---
title: "Data Science Capstone Week 2"
author: "R. Martinez"
date: "February 19, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is Capstone project for the Data Science Specialiazation of the John Hopkins University. The main objective is to build a text model to predict the next words that a user will type on a text device such a mobile text message app.  
  
This document shows the milestone report for week #2 where I will:   
- Get the data file to train the text model.  
- Perform data exploration to undestand the data.    
- Profile the data file to identify basic characteriscti of the data.  
- Provide a inital plan to build the prediction text model.  
  
## Library and Data  
  
The data was provided by the professor and can be downloaded from this source: <https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip>  
  
```{r data_load, warning=FALSE, comment="", cache=TRUE}
library(tm);    library(SnowballC); library(wordcloud);   library(dplyr) 
library(RWeka); library(ggplot2);   library(formattable); library(tokenizers)
set.seed(201902)

# Note that due the size of the data files (200MG, 196MG, and 159MG) 
# I will use smaller size of the files selecting lines randomly (using rbinom) 
file_path  <- "C:/Users/raulm/Documents/John Hopkins University/#10 Data Science Capstone/Coursera-SwiftKey/final/en_US/"
file_blogs <- "en_US.blogs.txt"
file_news  <- "en_US.news.txt"
file_twitter <- "en_US.twitter.txt"

# Load the "blogs" data file
incon <- file(paste(file_path, file_blogs ,sep=""),"r")
file <- readLines(incon, encoding="UTF-8", skipNul=TRUE)
data_blogs <- file[rbinom(length(file), 1, 0.01) == 1]
close(incon)
len_blogs <- length(file)
size_blogs <- object.size(file)
max_line_blogs <- max(nchar(file))
avg_line_blogs <- mean(nchar(file))
len_blogs_sample <- length(data_blogs)

# Load the "news" data file
incon <- file(paste(file_path, file_news ,sep=""),"rb")
file <- readLines(incon, encoding="UTF-8", skipNul=TRUE)
data_news <- file[rbinom(length(file), 1, 0.01) == 1]
close(incon)
len_news <- length(file)
size_news <- object.size(file)
max_line_news <- max(nchar(file))
avg_line_news <- mean(nchar(file))
len_news_sample <- length(data_news)

# Load the "twitter" data file
incon <- file(paste(file_path, file_twitter ,sep=""),"r")
file <- readLines(incon, encoding="UTF-8", skipNul=TRUE)
data_twitter <- file[rbinom(length(file), 1, 0.005) == 1]
close(incon)
len_twitter <- length(file)
size_twitter <- object.size(file)
max_line_twitter <- max(nchar(file))
avg_line_twitter <- mean(nchar(file))
len_twitter_sample <- length(data_twitter)

# Now remove the temp file to reselase memory
rm(file)
```
  
Once the data has been loaded, let's show basic metrics about the data files:    
  
```{r data_summary, warning=FALSE, comment="", cache=TRUE}
# Basic metrics about the files
data_metrics <- data.frame(file_name = c("en_US.blogs.txt","en_US.news.txt","en_US.twitter.txt"),
                         size = c(format(size_blogs, units = "auto"), 
                                  format(size_news, units = "auto"), 
                                  format(size_twitter, units = "auto")),
                         lines = c(format(len_blogs, big.mark=","),
                                   format(len_news, big.mark=","),
                                   format(len_twitter, big.mark=",")),
                         Average_line_length = c(round(avg_line_blogs,0), 
                                                 round(avg_line_news,0), 
                                                 round(avg_line_twitter,0)),
                         max_line_length = c(format(max_line_blogs, big.mark=","), 
                                             format(max_line_news, big.mark=","), 
                                             format(max_line_twitter, big.mark=","))
                         )
# summary table
colnames(data_metrics) <- c('File Name', 'File Size', 'Number of Lines', 'Average Length of Lines', 'Maimun Length of a line') 
formattable(data_metrics)
```

## Data Transformation

On this section I will prepare the data to make it ready for the prediction text model.  
To do that:
- Ensure the data is all ASCII.
- The text will be all in lowecase.
- Digits, puntuation, stopwords, white spaces will be removed.
- And the text will be transfomred to its stem form.    
    
```{r data_transformation, warning=FALSE, comment="", cache=TRUE}
# Make sure all characters are ASCII
data_blogs <- iconv(data_blogs,  to="ASCII", sub="")
data_news <- iconv(data_news,  to="ASCII", sub="")
data_twitter <- iconv(data_twitter,  to="ASCII", sub="")

# Create the corpus to use "tm" packages
my_corpus <- VCorpus(VectorSource(paste(data_blogs, data_news, data_twitter)))
# Remove original doc to release memory
rm(data_blogs, data_news, data_twitter)

# Transformation using "tm" functions
my_corpus <- tm_map(my_corpus, removeNumbers)
my_corpus <- tm_map(my_corpus, tolower)
my_corpus <- tm_map(my_corpus, removePunctuation)
my_corpus <- tm_map(my_corpus, removeWords, stopwords("english"))
my_corpus <- tm_map(my_corpus, stripWhitespace)
my_corpus <- tm_map(my_corpus, stemDocument)
my_corpus <- tm_map(my_corpus, PlainTextDocument)

# Just to inspect the corpus
inspect(my_corpus[1])
writeLines(as.character(my_corpus[1]))
```
  
  
## Data Exploration
    
On this section, I will show some data exploration made to the data that was all transfomed (previous section) into a **Corpus**.    
  
Let's start showing a **words cloud** with the top 200 most frequent words:

```{r word_cloud, warning=FALSE, comment="", cache=TRUE}  
wordcloud(my_corpus, max.words = 200, random.order = FALSE, colors=brewer.pal(8,"Dark2"))  
```
  
Now let's build the Ngrams tokenization to determine the most frequent uni-grams, bi-brams and three-grams:  

### Unigrams Tokenization:
  
```{r unigram, warning=FALSE, comment="", cache=TRUE}  
one_gramTokenizer <- function(x) NGramTokenizer(x=x, control=Weka_control(min = 1, max = 1))
one_dtm <- DocumentTermMatrix(my_corpus, control = list(tokenize = one_gramTokenizer))
# Because the matrix is too much sparse, let's find something that is not so much
one_dtm_sparse <- removeSparseTerms(one_dtm, sparse=0.99)

one_dtm_freq <- sort(colSums(as.matrix(one_dtm_sparse)),decreasing = TRUE)
one_dtm_freq_df <- data.frame(word = names(one_dtm_freq), frequency = one_dtm_freq)
head(one_dtm_freq_df, 10)

one_dtm_plot <- subset(one_dtm_freq_df, frequency > 2000)
ggplot(one_dtm_plot, aes(x=reorder(word, frequency), y=frequency)) +
       geom_bar(stat = "identity") +  coord_flip() +
       theme(legend.title=element_blank()) +
       xlab("1-Gram") + ylab("Frequency") +
       labs(title = "Frequency of 1-Grams > 2000")
```
  
### Bigrams Tokenization:

```{r bigram, warning=FALSE, comment="", cache=TRUE}  
two_gramTokenizer <- function(x) NGramTokenizer(x=x, control=Weka_control(min = 2, max = 2))
two_dtm <- DocumentTermMatrix(my_corpus, control = list(tokenize = two_gramTokenizer))
# Because the matrix is too much sparse, let's find something that is not so much
two_dtm_sparse <- removeSparseTerms(two_dtm, sparse=0.999)

two_dtm_freq <- sort(colSums(as.matrix(two_dtm_sparse)),decreasing = TRUE)
two_dtm_freq_df <- data.frame(word = names(two_dtm_freq), frequency = two_dtm_freq)
head(two_dtm_freq_df, 10)

two_dtm_plot <- subset(two_dtm_freq_df, frequency > 100)
ggplot(two_dtm_plot, aes(x=reorder(word, frequency), y=frequency)) +
        geom_bar(stat = "identity") +  coord_flip() +
        theme(legend.title=element_blank()) +
        xlab("2-Gram") + ylab("Frequency") +
        labs(title = "Frequency of 2-Grams > 100")
```
  
### Threegrams Tokenization:

```{r threegram, warning=FALSE, comment="", cache=TRUE}  
three_gramTokenizer <- function(x) NGramTokenizer(x=x, control=Weka_control(min = 3, max = 3))
three_dtm <- DocumentTermMatrix(my_corpus, control = list(tokenize = three_gramTokenizer))
# Because the matrix is too much sparse, let's find something that is not so much
three_dtm_sparse <- removeSparseTerms(three_dtm, sparse=0.9998)

three_dtm_freq <- sort(colSums(as.matrix(three_dtm_sparse)),decreasing = TRUE)
three_dtm_freq_df <- data.frame(word = names(three_dtm_freq), frequency = three_dtm_freq)
head(three_dtm_freq_df, 10)

three_dtm_plot <- subset(three_dtm_freq_df, frequency > 10)
ggplot(three_dtm_plot, aes(x=reorder(word, frequency), y=frequency)) +
        geom_bar(stat = "identity") +  coord_flip() +
        theme(legend.title=element_blank()) +
        xlab("3-Gram") + ylab("Frequency") +
        labs(title = "Frequency of 3-Grams > 10")
```  
  
  
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
  
.  
.  
.  